<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=yes, viewport-fit=cover"
    />
    <link rel="stylesheet" href="../../navbar.css" />
    <link rel="stylesheet" href="./all-projects.css" />
    <link
      rel="icon"
      type="image/x-icon"
      href="../assets/images/logos/favicon.ico"
    />
    <title>Intermittent</title>
  </head>

  <!-- Google Analytics -->
  <!-- Google tag (gtag.js) -->
  <script
    async
    src="https://www.googletagmanager.com/gtag/js?id=G-6F7Y5HSK0Z"
  ></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());

    gtag("config", "G-6F7Y5HSK0Z");
  </script>

  <body>
    <!-- Project Visual -->
    <div class="project-visual">
      <img src="../assets/images/thumbnails/intermittent.png" id="main-img" />
    </div>

    <!-- Project Title -->
    <div class="title">
      <h1>Intermittent</h1>
    </div>

    <div class="project-container">
      <!-- Project Details -->
      <div class="details">
        <div class="keywords">
          <h2 style="color: #796cd9">StyleGANs</h2>
          <h2 style="color: #4ec4c8">Image Generation</h2>
          <h2 style="color: #65cf95">Light and shadow</h2>
        </div>

        <!-- Tools Used Section -->
        <div class="tools-container">
          <p>
            <span id="toolkit-title">TOOLKIT</span>
            <span>Python</span>
            <span>Google Colab</span>
            <span>Hugging Face</span>
          </p>
        </div>
      </div>

      <!-- Project Documentation -->
      <div class="documentation">
        <h3>StyleGANs</h3>
        <p>
          StyleGAN is a Generative Adversarial Network which is used to train
          the model to generate the images. StyleGANs consist of a generator and
          discriminator.
        </p>
        <p>
          The generator takes random noise as input and transforms it into
          images. In the case of StyleGAN, the generator produces images in a
          step-by-step manner, starting from a low-resolution image and
          gradually adding details to create a high-resolution image. It does
          this by learning the underlying patterns and structures present in a
          dataset of images during training.
        </p>
        <p>
          The styleGAN discriminator distinguishes between real images from the
          dataset and fake images generated by the generator. It learns to
          classify images as real or fake by comparing them to the images it has
          seen during training. It can choose an image and generate it based on
          how it's been trained. I decided to incorporate the images generated
          through different phases of image model training as the basis for
          creating my poster designs.
        </p>
        <h3>Image Generation</h3>
        <p>
          The first step in making this project was curating a dataset of
          images. I wanted to create monochrome and duochrome visuals based on
          light and dark places so I began to look for images that fit the theme
          through my archive of images.
        </p>
        <p>
          The next step was running Python code on a Google Colab notebook to
          start training the model. It takes a long time to generate the
          different iterations of images also known as seeds. I did about ten
          rounds of generation.
        </p>
        <h3>Text Generation</h3>
        <p>
          As for generating the textual elements that went on the poster, I
          sourced quotes from the book "All the Light We Cannot See" by Anthony
          Doerr. I was watching the show that came out about the book and was
          really intrigued by it at the time. I used spaCy as part of my Python
          code to recognise different parts of speech within the text and then
          construct custom generated phrases out of them. Furthermore, I used
          Hugging Face's Mistral-7B-v0.1 text generation model to generate text
          based on certain input prompts.
        </p>
        <div class="inline-img-container">
          <figure>
            <img
              class="inline-img"
              src="../assets/images/project-pages/intermittent/1.png"
            />
            <div></div>
            <div></div>
            <div></div>
            <div></div>
          </figure>
        </div>
        <h3>Sources</h3>
        <p>
          Text Source - All the Light We Cannot See by Anthony Doerr <br />
          Images source - All images taken by me
        </p>
      </div>
    </div>

    <!-- Next Previous Buttons Section -->
    <div class="nextPrev">
      <div class="prev-project">
        <a href="./echoes.html" class="previous">&#8249;</a>
        <p>Previous Project</p>
      </div>
      <div class="next-project">
        <p>Next Project</p>
        <a href="./temperature-of-emotions.html" class="next">&#8250;</a>
      </div>
    </div>
    <hr />

    <script src="./projects.js"></script>
    <script src="../navbar.js"></script>
  </body>
</html>
